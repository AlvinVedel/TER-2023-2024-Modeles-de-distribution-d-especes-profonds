print(paste("faux positifs : ", faux_positifs))
faux_negatifs <- confusion_mat[1, 2]
print(paste("faux négatifs : ", faux_negatifs))
vrais_positifs <- confusion_mat[2, 2]
vrais_negatifs <- confusion_mat[1, 1]
rappel_rl <- vrais_positifs / (vrais_positifs+faux_negatifs)
precision_rl <- vrais_positifs / (vrais_positifs+faux_positifs)
specificite_rl <- vrais_negatifs / (vrais_negatifs+faux_positifs)
F1_score_rl <- 2*vrais_positifs / (2*vrais_positifs + faux_positifs + faux_negatifs)
print(paste("Rappel : ", rappel_rl))
print(paste("Precision : ", precision_rl))
print(paste("F1 score : ", F1_score_rl))
print(paste("Specificite : ", specificite_rl))
for(i in c(0.1, 0.25,0.5, 0.75, 0.9)){
seuil=i
predictions_01 = predictions > seuil
confusion_mat <- table(predit = predictions_01, reel = prostate$lymph)
print(paste("seuil : ", seuil))
print(confusion_mat)
}
library(rpart)
# construction de l'arbre
tree.fit <- rpart(lymph ~ age + acid + radio + gravite + taille, data = prostate)
# affichage de l'arborescence
plot(tree.fit)
# affichage des règles de décisions
text(tree.fit)
# L'arbre proposé est le résultat d'un élagage obtenu par une procédure intégrée de validation croisée
# ci dessous, quelques commandes pour obtenir les valeurs de ce critère :
printcp(tree.fit)  # Prints a table of optimal prunings based on a Complexity Parameter (cp).
plotcp(tree.fit)
help(rpart)
# utiliser la fonction predict()
predict(tree.fit, type = "prob")   # class probabilities (default)
predict(tree.fit, type = "vector") # level numbers
predict(tree.fit, type = "class")  # factor
predict(tree.fit, type = "matrix") # level number, class frequencies, probabilities
# tableau des effectifs dans les données
table(prostate$lymph)
# tableau des effectifs dans les prédictions
table(predict(tree.fit, type = "class"))
# matrice de confusion
confusion_mat=table(predict(tree.fit, type = "class"),prostate$lymph)
print(confusion_mat)
TN= confusion_mat[1,1]
FN= confusion_mat[1,2]
FP= confusion_mat[2,1]
TP= confusion_mat[2,2]
print(TN)
# taux de bien classés au global
accuracy = (TP+TN)/sum(confusion_mat)
# taux de bien classés parmi les prédits positifs
precision = TP/(TP+FP)
# taux d'exemples positifs bien classés (quelle part a t-on détecté parmi les positifs)
rappel = TP/(TP+FN)
#affichage
print(paste("accuracy = ", round(accuracy,2)))
print(paste("precision = ",round(precision,2) ))
print(paste("rappel = ",round(rappel,2) ))
## affichage amélioré avec le package 'rpart.plot'
library(rpart.plot)
rpart.plot(tree.fit)
comparaison_precision <- precision/precision_rl
comparaison_rappel <- rappel/rappel_rl
print(paste("la rapport précision arbre / précision regression est : ", comparaison_precision))
print(paste("la rapport rappel arbre / rappel regression est : ", comparaison_rappel))
fit_logistic2 = glm(lymph ~ log(acid) + radio + taille, family = "binomial" ,data = prostate)
predictions_2 = predict(fit_logistic2,type = "response")
seuil=1/2
predictions_01_2 = predictions > seuil
confusion_mat2 <- table(predit = predictions_01_2, reel = prostate$lymph)
faux_positifs2 <- confusion_mat2[2, 1]
faux_negatifs2 <- confusion_mat2[1, 2]
vrais_positifs2 <- confusion_mat2[2, 2]
vrais_negatifs2 <- confusion_mat2[1, 1]
rappel_rl2 <- vrais_positifs2 / (vrais_positifs2+faux_negatifs2)
precision_rl2 <- vrais_positifs2 / (vrais_positifs2+faux_positifs2)
specificite_rl2 <- vrais_negatifs2 / (vrais_negatifs2+faux_positifs2)
F1_score_rl2 <- 2*vrais_positifs2 / (2*vrais_positifs2 + faux_positifs2 + faux_negatifs2)
print(paste("Rappel : ", rappel_rl2))
print(paste("Précision : ", precision_rl2))
fit_logistic2 = glm(lymph ~ log(acid) + radio + taille, family = "binomial" ,data = prostate)
predictions_2 = predict(fit_logistic2,type = "response")
seuil=1/2
predictions_01_2 = predictions > seuil
confusion_mat2 <- table(predit = predictions_01_2, reel = prostate$lymph)
faux_positifs2 <- confusion_mat2[2, 1]
faux_negatifs2 <- confusion_mat2[1, 2]
vrais_positifs2 <- confusion_mat2[2, 2]
vrais_negatifs2 <- confusion_mat2[1, 1]
rappel_rl2 <- vrais_positifs2 / (vrais_positifs2+faux_negatifs2)
precision_rl2 <- vrais_positifs2 / (vrais_positifs2+faux_positifs2)
specificite_rl2 <- vrais_negatifs2 / (vrais_negatifs2+faux_positifs2)
F1_score_rl2 <- 2*vrais_positifs2 / (2*vrais_positifs2 + faux_positifs2 + faux_negatifs2)
print(paste("Rappel : ", rappel_rl2))
print(paste("Précision : ", precision_rl2))
if(rappel_rl2 <= rappel_rl){
print("la réduction de variable n'a pas amélioré le rappel du modèle")
}else{
print("La réduction de variable a augmenté le rappel")}
fit_logistic2 = glm(lymph ~ log(acid) + radio + taille, family = "binomial" ,data = prostate)
predictions_2 = predict(fit_logistic2,type = "response")
seuil=1/2
predictions_01_2 = predictions > seuil
confusion_mat2 <- table(predit = predictions_01_2, reel = prostate$lymph)
faux_positifs2 <- confusion_mat2[2, 1]
faux_negatifs2 <- confusion_mat2[1, 2]
vrais_positifs2 <- confusion_mat2[2, 2]
vrais_negatifs2 <- confusion_mat2[1, 1]
rappel_rl2 <- vrais_positifs2 / (vrais_positifs2+faux_negatifs2)
precision_rl2 <- vrais_positifs2 / (vrais_positifs2+faux_positifs2)
specificite_rl2 <- vrais_negatifs2 / (vrais_negatifs2+faux_positifs2)
F1_score_rl2 <- 2*vrais_positifs2 / (2*vrais_positifs2 + faux_positifs2 + faux_negatifs2)
print(paste("Rappel : ", rappel_rl2))
print(paste("Précision : ", precision_rl2))
if(rappel_rl2 <= rappel_rl){
print("la réduction de variable n'a pas amélioré le rappel du modèle")
}else{
print("La réduction de variable a augmenté le rappel")}
if(precision_rl2 <= precision_rl){
print("la réduction de variable n'a pas amélioré la précision du modèle")
}else{
print("La réduction de variable a augmenté la précision")}
?rpart
tree.fit2 <- rpart(lymph ~ age + acid + radio + gravite + taille, data = prostate,
control = rpart.control(cp = 0.05))
plot(tree.fit)
# affichage des règles de décisions
text(tree.fit)
# L'arbre proposé est le résultat d'un élagage obtenu par une procédure intégrée de validation croisée
# ci dessous, quelques commandes pour obtenir les valeurs de ce critère :
printcp(tree.fit)  # Prints a table of optimal prunings based on a Complexity Parameter (cp).
plotcp(tree.fit)
tree.fit2 <- rpart(lymph ~ age + acid + radio + gravite + taille, data = prostate,
control = rpart.control(cp = 0.05))
plot(tree.fit2)
# affichage des règles de décisions
text(tree.fit2)
# L'arbre proposé est le résultat d'un élagage obtenu par une procédure intégrée de validation croisée
# ci dessous, quelques commandes pour obtenir les valeurs de ce critère :
printcp(tree.fit2)  # Prints a table of optimal prunings based on a Complexity Parameter (cp).
plotcp(tree.fit2)
tree.fit2 <- rpart(lymph ~ age + acid + radio + gravite + taille, data = prostate,
control = rpart.control(cp = 0.00001))
plot(tree.fit2)
# affichage des règles de décisions
text(tree.fit2)
# L'arbre proposé est le résultat d'un élagage obtenu par une procédure intégrée de validation croisée
# ci dessous, quelques commandes pour obtenir les valeurs de ce critère :
printcp(tree.fit2)  # Prints a table of optimal prunings based on a Complexity Parameter (cp).
plotcp(tree.fit2)
tree.fit2 <- rpart(lymph ~ age + acid + radio + gravite + taille, data = prostate,
control = rpart.control(cp = 0.2))
plot(tree.fit2)
# affichage des règles de décisions
text(tree.fit2)
# L'arbre proposé est le résultat d'un élagage obtenu par une procédure intégrée de validation croisée
# ci dessous, quelques commandes pour obtenir les valeurs de ce critère :
printcp(tree.fit2)  # Prints a table of optimal prunings based on a Complexity Parameter (cp).
plotcp(tree.fit2)
tree.fit2 <- rpart(lymph ~ age + acid + radio + gravite + taille, data = prostate,
control = rpart.control(cp = 0.00000001))
plot(tree.fit2)
# affichage des règles de décisions
text(tree.fit2)
# L'arbre proposé est le résultat d'un élagage obtenu par une procédure intégrée de validation croisée
# ci dessous, quelques commandes pour obtenir les valeurs de ce critère :
printcp(tree.fit2)  # Prints a table of optimal prunings based on a Complexity Parameter (cp).
plotcp(tree.fit2)
tree.fit2 <- rpart(lymph ~ age + acid + radio + gravite + taille, data = prostate,
control = rpart.control(cp = 0.5))
plot(tree.fit2)
tree.fit2 <- rpart(lymph ~ age + acid + radio + gravite + taille, data = prostate,
control = rpart.control(cp = 0.2))
plot(tree.fit2)
# affichage des règles de décisions
text(tree.fit2)
# L'arbre proposé est le résultat d'un élagage obtenu par une procédure intégrée de validation croisée
# ci dessous, quelques commandes pour obtenir les valeurs de ce critère :
printcp(tree.fit2)  # Prints a table of optimal prunings based on a Complexity Parameter (cp).
plotcp(tree.fit2)
tree.fit2 <- rpart(lymph ~ age + acid + radio + gravite + taille, data = prostate,
control = rpart.control(minsplit = 3, cp = 0.05))
plot(tree.fit2)
# affichage des règles de décisions
text(tree.fit2)
# L'arbre proposé est le résultat d'un élagage obtenu par une procédure intégrée de validation croisée
# ci dessous, quelques commandes pour obtenir les valeurs de ce critère :
printcp(tree.fit2)  # Prints a table of optimal prunings based on a Complexity Parameter (cp).
plotcp(tree.fit2)
tree.fit2 <- rpart(lymph ~ age + acid + radio + gravite + taille, data = prostate,
control = rpart.control(minsplit = 5, cp = 0.05))
plot(tree.fit2)
# affichage des règles de décisions
text(tree.fit2)
# L'arbre proposé est le résultat d'un élagage obtenu par une procédure intégrée de validation croisée
# ci dessous, quelques commandes pour obtenir les valeurs de ce critère :
printcp(tree.fit2)  # Prints a table of optimal prunings based on a Complexity Parameter (cp).
plotcp(tree.fit2)
tree.fit2 <- rpart(lymph ~ age + acid + radio + gravite + taille, data = prostate,
control = rpart.control(minsplit = 3, cp = 0.05))
plot(tree.fit2)
# affichage des règles de décisions
text(tree.fit2)
# L'arbre proposé est le résultat d'un élagage obtenu par une procédure intégrée de validation croisée
# ci dessous, quelques commandes pour obtenir les valeurs de ce critère :
printcp(tree.fit2)  # Prints a table of optimal prunings based on a Complexity Parameter (cp).
plotcp(tree.fit2)
tree.fit2 <- rpart(lymph ~ age + acid + radio + gravite + taille, data = prostate,
control = rpart.control(minsplit = 4, cp = 0.00001))
plot(tree.fit2)
# affichage des règles de décisions
text(tree.fit2)
# L'arbre proposé est le résultat d'un élagage obtenu par une procédure intégrée de validation croisée
# ci dessous, quelques commandes pour obtenir les valeurs de ce critère :
printcp(tree.fit2)  # Prints a table of optimal prunings based on a Complexity Parameter (cp).
plotcp(tree.fit2)
tree.fit2 <- rpart(lymph ~ age + acid + radio + gravite + taille, data = prostate,
control = rpart.control(minsplit = 4, cp = 0.001))
plot(tree.fit2)
# affichage des règles de décisions
text(tree.fit2)
# L'arbre proposé est le résultat d'un élagage obtenu par une procédure intégrée de validation croisée
# ci dessous, quelques commandes pour obtenir les valeurs de ce critère :
printcp(tree.fit2)  # Prints a table of optimal prunings based on a Complexity Parameter (cp).
plotcp(tree.fit2)
tree.fit2 <- rpart(lymph ~ age + acid + radio + gravite + taille, data = prostate,
control = rpart.control(minsplit = 4, cp = 0.001))
plot(tree.fit2)
# affichage des règles de décisions
text(tree.fit2)
# L'arbre proposé est le résultat d'un élagage obtenu par une procédure intégrée de validation croisée
# ci dessous, quelques commandes pour obtenir les valeurs de ce critère :
printcp(tree.fit2)  # Prints a table of optimal prunings based on a Complexity Parameter (cp).
plotcp(tree.fit2)
confusion_mat3=table(predict(tree.fit, type = "class"),prostate$lymph)
TN= confusion_mat[1,1]
FN= confusion_mat[1,2]
FP= confusion_mat[2,1]
TP= confusion_mat[2,2]
precision_2 = TP/(TP+FP)
# taux d'exemples positifs bien classés (quelle part a t-on détecté parmi les positifs)
rappel_2 = TP/(TP+FN)
if(precision_2<=precision){
print("Le modèle avec davantage de noeud n'a pas amélioré la précision")
} else { print("Le modèle a améliorer la précision")}
if(rappel_2<=rappel){
print("Le modèle avec davantage de noeud n'a pas amélioré le rappel")
} else { print("Le modèle a améliorer le rappel")}
rpart.plot(tree.fit2)
tree.fit2 <- rpart(lymph ~ age + acid + radio + gravite + taille, data = prostate,
control = rpart.control(minsplit = 3, cp = 0.001))
plot(tree.fit2)
# affichage des règles de décisions
text(tree.fit2)
# L'arbre proposé est le résultat d'un élagage obtenu par une procédure intégrée de validation croisée
# ci dessous, quelques commandes pour obtenir les valeurs de ce critère :
printcp(tree.fit2)  # Prints a table of optimal prunings based on a Complexity Parameter (cp).
plotcp(tree.fit2)
confusion_mat3=table(predict(tree.fit, type = "class"),prostate$lymph)
TN= confusion_mat[1,1]
FN= confusion_mat[1,2]
FP= confusion_mat[2,1]
TP= confusion_mat[2,2]
precision_2 = TP/(TP+FP)
# taux d'exemples positifs bien classés (quelle part a t-on détecté parmi les positifs)
rappel_2 = TP/(TP+FN)
if(precision_2<=precision){
print("Le modèle avec davantage de noeud n'a pas amélioré la précision")
} else { print("Le modèle a améliorer la précision")}
if(rappel_2<=rappel){
print("Le modèle avec davantage de noeud n'a pas amélioré le rappel")
} else { print("Le modèle a améliorer le rappel")}
rpart.plot(tree.fit2)
tree.fit2 <- rpart(lymph ~ age + acid + radio + gravite + taille, data = prostate,
control = rpart.control(minsplit = 3, cp = 0.05))
plot(tree.fit2)
# affichage des règles de décisions
text(tree.fit2)
# L'arbre proposé est le résultat d'un élagage obtenu par une procédure intégrée de validation croisée
# ci dessous, quelques commandes pour obtenir les valeurs de ce critère :
printcp(tree.fit2)  # Prints a table of optimal prunings based on a Complexity Parameter (cp).
plotcp(tree.fit2)
confusion_mat3=table(predict(tree.fit, type = "class"),prostate$lymph)
TN= confusion_mat[1,1]
FN= confusion_mat[1,2]
FP= confusion_mat[2,1]
TP= confusion_mat[2,2]
precision_2 = TP/(TP+FP)
# taux d'exemples positifs bien classés (quelle part a t-on détecté parmi les positifs)
rappel_2 = TP/(TP+FN)
if(precision_2<=precision){
print("Le modèle avec davantage de noeud n'a pas amélioré la précision")
} else { print("Le modèle a améliorer la précision")}
if(rappel_2<=rappel){
print("Le modèle avec davantage de noeud n'a pas amélioré le rappel")
} else { print("Le modèle a améliorer le rappel")}
rpart.plot(tree.fit2)
# On constate un rang de 3 pour 4 colonnes == pas de plein rang
# Cela suffit à invalider le modèle mais on continue l'analyse, on va rajouter un bruit à la deuxième variable pour que la colinéarité ne soit pas strictes :
bruit <- rnorm(50, 0, 0.5)
bruit
x3 <- x3+bruit
# On doit d'abord vérifier l'hypothèse 1 : la matrice X est de plein rang
qr(X)$rank
# On constate un rang de 3 pour 4 colonnes == pas de plein rang
# Cela suffit à invalider le modèle mais on continue l'analyse, on va rajouter un bruit à la deuxième variable pour que la colinéarité ne soit pas strictes :
bruit <- rnorm(50, 0, 0.5)
x3 <- x3+bruit
X <- cbind(vec1, x1, x2, x3)
# vecteur Beta des paramètres : je les crée moi même au lieu de générer un Y aléatoire
Beta <- c(5, 1, 2, -0.5)
# On crée la colonne des Y à partir de notre jeu de données et les paramètres choisis
y <- X %*% Beta + erreur
# le dataset A correspond au jeu de donnée initial avec les valeurs pour les différentes variables explicatives et la variable expliquée
datasetA <- cbind(X[, 2:4], as.data.frame(y))
head(datasetA)
plot(datasetA)
cor_mat=cor(datasetA[,1:3])
library(corrplot)
# on affiche la matrice des corrélations, on observe une forte corrélation entre x2 et x3
corrplot(cor_mat, type="upper", order="hclust", tl.col="black", tl.srt=45)
# Génération du jeu de donnée 3 variables  -> échantillon taille n=50
x1 <- rnorm(50, 4, 1)
x2 <- rnorm(50, 2, 0.5)
# Introduction variable ne respectant pas les hypothèses : x3 combinaison linéaires des 2 autres donc
# matrice X pas de plein rang
x3 <- -2*x2
# Génération du vecteur des erreurs (respecte hypothèse sur les erreurs)
erreur <- rnorm(50, 0, 1)
# Création du vecteur des 1
vec1 <- rep(1, 50)
# matrice X des régresseurs
X <- cbind(vec1, x1, x2, x3)
# vecteur Beta des paramètres : je les crée moi même au lieu de générer un Y aléatoire
Beta <- c(5, 1, 2, -0.5)
# On crée la colonne des Y à partir de notre jeu de données et les paramètres choisis
y <- X %*% Beta + erreur
# le dataset A correspond au jeu de donnée initial avec les valeurs pour les différentes variables explicatives et la variable expliquée
datasetA <- cbind(X[, 2:4], as.data.frame(y))
head(datasetA)
plot(datasetA)
# On doit d'abord vérifier l'hypothèse 1 : la matrice X est de plein rang
qr(X)$rank
# On constate un rang de 3 pour 4 colonnes == pas de plein rang
# Cela suffit à invalider le modèle mais on continue l'analyse, on va rajouter un bruit à la deuxième variable pour que la colinéarité ne soit pas strictes :
bruit <- rnorm(50, 0, 0.5)
x3 <- x3+bruit
X <- cbind(vec1, x1, x2, x3)
# vecteur Beta des paramètres : je les crée moi même au lieu de générer un Y aléatoire
Beta <- c(5, 1, 2, -0.5)
# On crée la colonne des Y à partir de notre jeu de données et les paramètres choisis
y <- X %*% Beta + erreur
# le dataset A correspond au jeu de donnée initial avec les valeurs pour les différentes variables explicatives et la variable expliquée
datasetA <- cbind(X[, 2:4], as.data.frame(y))
head(datasetA)
plot(datasetA)
cor_mat=cor(datasetA[,1:3])
library(corrplot)
# on affiche la matrice des corrélations, on observe une forte corrélation entre x2 et x3
corrplot(cor_mat, type="upper", order="hclust", tl.col="black", tl.srt=45)
# on propose un modèle avec toutes les variables
reg <- lm(V1~. , data = datasetA)
summary(reg)
# On observe que la 3ème variable n'est pas significative (comme attendu)
plot(reg$residuals)
abline(h=0)
# les résidus pour leur part sont bien distribués, selon une loi normale 0, 1
# légère dispersion des valeurs parfois vers le 2 ou le -2 -> fluctuation d'échantillonage
cor_mat
eigen(cor_mat)
val_p$values[1]/val_p$values
# On propose de calculer l'indice de conditionnement K pour vérifier si la colinéarité n'est pas trop forte
val_p <- eigen(cor_mat)
val_p$values[1]/val_p$values
# Génération du jeu de donnée 3 variables  -> échantillon taille n=50
x1 <- rnorm(100, 4, 2)
x2 <- rnorm(100, 2, 1)
# Introduction variable ne respectant pas les hypothèses : x3 combinaison linéaires des 2 autres donc
# matrice X pas de plein rang
x3 <- -2*x2
# Génération du vecteur des erreurs (respecte hypothèse sur les erreurs)
erreur <- rnorm(100, 0, 3)
# Création du vecteur des 1
vec1 <- rep(1, 100)
# matrice X des régresseurs
X <- cbind(vec1, x1, x2, x3)
# vecteur Beta des paramètres : je les crée moi même au lieu de générer un Y aléatoire
Beta <- c(5, 1, 2, -0.5)
# On crée la colonne des Y à partir de notre jeu de données et les paramètres choisis
y <- X %*% Beta + erreur
# le dataset A correspond au jeu de donnée initial avec les valeurs pour les différentes variables explicatives et la variable expliquée
datasetA <- cbind(X[, 2:4], as.data.frame(y))
head(datasetA)
plot(datasetA)
# On doit d'abord vérifier l'hypothèse 1 : la matrice X est de plein rang
qr(X)$rank
# On constate un rang de 3 pour 4 colonnes == pas de plein rang
# Cela suffit à invalider le modèle mais on continue l'analyse, on va rajouter un bruit à la deuxième variable pour que la colinéarité ne soit pas strictes :
bruit <- rnorm(100, 0, 0.1)
x3 <- x3+bruit
X <- cbind(vec1, x1, x2, x3)
# vecteur Beta des paramètres : je les crée moi même au lieu de générer un Y aléatoire
Beta <- c(5, 1, 2, -0.5)
# On crée la colonne des Y à partir de notre jeu de données et les paramètres choisis
y <- X %*% Beta + erreur
# le dataset A correspond au jeu de donnée initial avec les valeurs pour les différentes variables explicatives et la variable expliquée
datasetA <- cbind(X[, 2:4], as.data.frame(y))
head(datasetA)
plot(datasetA)
# Génération du jeu de donnée 3 variables  -> échantillon taille n=50
x1 <- rnorm(100, 4, 2)
x2 <- rnorm(100, 2, 1)
# Introduction variable ne respectant pas les hypothèses : x3 combinaison linéaires des 2 autres donc
# matrice X pas de plein rang
x3 <- -2*x2
# Génération du vecteur des erreurs (respecte hypothèse sur les erreurs)
erreur <- rnorm(100, 0, 3)
# Création du vecteur des 1
vec1 <- rep(1, 100)
# matrice X des régresseurs
X <- cbind(vec1, x1, x2, x3)
# vecteur Beta des paramètres : je les crée moi même au lieu de générer un Y aléatoire
Beta <- c(5, 1, 2, -0.5)
# On crée la colonne des Y à partir de notre jeu de données et les paramètres choisis
y <- X %*% Beta + erreur
# le dataset A correspond au jeu de donnée initial avec les valeurs pour les différentes variables explicatives et la variable expliquée
datasetA <- cbind(X[, 2:4], as.data.frame(y))
head(datasetA)
plot(datasetA)
# On doit d'abord vérifier l'hypothèse 1 : la matrice X est de plein rang
qr(X)$rank
# On constate un rang de 3 pour 4 colonnes == pas de plein rang
# Cela suffit à invalider le modèle mais on continue l'analyse, on va rajouter un bruit à la deuxième variable pour que la colinéarité ne soit pas strictes :
bruit <- rnorm(100, 0, 0.1)
x3 <- x3+bruit
X <- cbind(vec1, x1, x2, x3)
# le dataset A correspond au jeu de donnée initial avec les valeurs pour les différentes variables explicatives et la variable expliquée
datasetA <- cbind(X[, 2:4], as.data.frame(y))
head(datasetA)
plot(datasetA)
cor_mat=cor(datasetA[,1:3])
library(corrplot)
# on affiche la matrice des corrélations, on observe une forte corrélation entre x2 et x3
corrplot(cor_mat, type="upper", order="hclust", tl.col="black", tl.srt=45)
# On propose de calculer l'indice de conditionnement K pour vérifier si la colinéarité n'est pas trop forte
val_p <- eigen(cor_mat)
val_p$values[1]/val_p$values
qr(X)$rank
knitr::opts_chunk$set(echo = TRUE)
selected <- sample(liste, replace=F, 3)
liste <- c("Ambre", 'Alvin", "Johnny", "Mathis", "Theo", "Beb")
selected <- sample(liste, replace=F, 3)
selected
liste <- c("Ambre", "Alvin", "Johnny", "Mathis", "Theo", "Beb")
selected <- sample(liste, replace=F, 3)
priselected
liste <- c("Ambre", "Alvin", "Johnny", "Mathis", "Theo", "Beb")
selected <- sample(liste, replace=F, 3)
print(selected)
liste <- c("Ambre", "Alvin", "Johnny", "Mathis", "Theo", "Beb")
selected <- sample(liste,size=3, replace=F)
print(selected)
selected <- sample(liste,size=3, replace=F)
print(selected)
selected <- sample(liste,size=3, replace=F)
print(selected)
print(paste("1er tour : ", selected)
print(paste("1er tour : ", selected))
liste <- c("Ambre", "Alvin", "Johnny", "Mathis", "Theo", "Beb")
selected <- sample(liste,size=3, replace=F)
print(paste("1er tour : ", selected))
liste <- c("Ambre", "Alvin", "Johnny", "Mathis", "Theo", "Beb")
selected <- sample(liste,size=3, replace=F)
print(paste("1er tour : ", selected))
liste <- c("Ambre", "Alvin", "Johnny", "Mathis", "Theo", "Beb")
selected <- sample(liste,size=3, replace=F)
print(selected)
liste <- c("Ambre", "Alvin", "Johnny", "Mathis", "Theo", "Beb")
selected <- sample(liste,size=3, replace=F)
print(selected)
liste <- c("Ambre", "Alvin", "Johnny", "Mathis", "Theo", "Beb")
selected <- sample(liste,size=3, replace=F)
print(selected)
liste <- c("Ambre", "Alvin", "Johnny", "Mathis", "Theo", "Beb")
selected <- sample(liste,size=3, replace=F)
print(selected)
liste <- c("Ambre", "Alvin", "Johnny", "Mathis", "Theo", "Beb")
selected <- sample(liste,size=3, replace=F)
print(selected)
liste <- c("Ambre", "Alvin", "Johnny", "Mathis", "Theo", "Beb")
selected <- sample(liste,size=3, replace=F)
print(selected)
liste <- c("Ambre", "Alvin", "Johnny", "Mathis", "Theo", "Beb")
selected <- sample(liste,size=3, replace=F)
print(selected)
setwd("C:/Users/Utilisateur/Cours/Master/TER-2023-2024-Modeles-de-distribution-d-especes-profonds/Codes/Visu_R")
df <- read.csv("../Data/Data_train/abiodata_speciesID.csv")
df <- read.csv("../Data/Data_train/abiodata_speciesID.csv", header=T)
df <- read.csv("..//Data//Data_train//abiodata_speciesID.csv", header=T)
df <- read.csv("..\Data\Data_train\abiodata_speciesID.csv", header=T)
df <- read.csv("C:\Users\Utilisateur\Cours\Master\TER-2023-2024-Modeles-de-distribution-d-especes-profonds\Data\Data_train\abiodata_speciesID.csv", header=T)
df <- read.csv("..\\Data\\Data_train\\abiodata_speciesID.csv", header=T)
df <- read.csv("C:\\Users\\Utilisateur\\Cours\\Master\\TER-2023-2024-Modeles-de-distribution-d-especes-profonds\\Data\\Data_train\\abiodata_speciesID.csv", header=T)
df <- read.csv("C:\\Users\\Utilisateur\\Cours\\Master\\TER-2023-2024-Modeles-de-distribution-d-especes-profonds\\Data\\Data_train\\abiodata_speciesID.csv", header=T, sep=',')
df <- read.csv("C:\\Users\\Utilisateur\\Cours\\Master\\TER-2023-2024-Modeles-de-distribution-d-especes-profonds\\Data\\Data_train\\abiodata_speciesID.csv", header=T, sep=';')
head(df)
df <- read.csv("C:\\Users\\Utilisateur\\Downloads\\presence_1_0_par_patch.csv", header=T, sep=';')
head(df)
df <- read.csv("C:\\Users\\Utilisateur\\Downloads\\presence_1_0_par_patch.csv", header=T, sep=',')
df <- read.csv("C:\\Users\\Utilisateur\\Downloads\\presence_1_0_par_patch.csv", header=T, sep=',')
head(df)
